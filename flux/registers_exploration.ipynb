{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Add parent directory to sys.path\n",
    "parent_dir = Path.cwd().parent.resolve()\n",
    "if str(parent_dir) not in sys.path:\n",
    "    sys.path.insert(0, str(parent_dir))\n",
    "\n",
    "# Verify that the path has been added correctly\n",
    "print(sys.path[0])\n",
    "\n",
    "from diffusers import FluxPipeline\n",
    "from diffusers.models import AutoencoderTiny\n",
    "from SDLens import HookedFluxPipeline, HookedPixArtPipeline\n",
    "from SAE import SparseAutoencoder\n",
    "import torch\n",
    "import os\n",
    "\n",
    "os.environ['HF_HOME'] = '/dlabscratch1/anmari'\n",
    "os.environ['TRANSFORMERS_CACHE'] = '/dlabscratch1/anmari'\n",
    "os.environ['HF_DATASETS_CACHE'] = '/dlabscratch1/anmari'\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from importlib import reload\n",
    "from visualization import plot_activation_by_layer, plot_activation_by_layer_og_ablated, interactive_image_activation, norm_tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = HookedPixArtPipeline.from_pretrained(\n",
    "    \"PixArt-alpha/PixArt-Sigma-XL-2-1024-MS\", \n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"balanced\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latents = pipe.pipe(\n",
    "    prompt=PROMPT_PIRATE,\n",
    "    num_images_per_prompt=1,\n",
    ")\n",
    "latents.images[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Pipeline\n",
    "from flux.utils import *\n",
    "\n",
    "\n",
    "dtype = torch.float16\n",
    "pipe = HookedFluxPipeline.from_pretrained(\n",
    "    \"black-forest-labs/FLUX.1-schnell\",\n",
    "    torch_dtype=dtype,\n",
    "    device_map=\"balanced\",\n",
    ")\n",
    "pipe.set_progress_bar_config(disable=True)\n",
    "set_flux_context(pipe, dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompts to experiment with\n",
    "SPACE_PROMPT = \"A sheep riding a cow in the space, there are planets and stars in the background.\"\n",
    "CACTUS_SKATEBOARD_PROMPT = \"A cinematic shot of a cactus on a skateboard in a hotel room.\"\n",
    "GIRL_CAT_PROMPT = \"A picture of a smiling girl with a red t-shirt holding a cat.\"\n",
    "PROMPT_PIRATE = \"A pirate with red hair smiling to the camera.\"\n",
    "PROMPT_SINGER = \"A lady singer with red hair smiling to the camera.\"\n",
    "PROMPT_COUPLE = \"A couple of students smiling to the camera.\"\n",
    "PROMPT_DOG = \"An happy husky looking at the camera with a bone in his mouth.\"\n",
    "PROMPT_CARTOON = \"An cartoonish picture of two smiling students, a male on the lft with a blue shirt and a black backpack, a female on the right has a yellow pullover\"\n",
    "EMPTY_PROMPT = \"\"\n",
    "\n",
    "REFINED_SPACE_PROMPT = \"A sheep riding a cow in the space, there is a school in background.\"\n",
    "REFINED_CACTUS_SKATEBOARD_PROMPT = \"A cinematic shot of a cactus on a skateboard.\"\n",
    "REFINED_GIRL_CAT_PROMPT = \"A picture of a smiling girl with a red t-shirt holding a cat in a park.\"\n",
    "REFINED_PROMPT_PIRATE = \"A pirate with red hair smiling to the camera on a pirate sheep.\"\n",
    "REFINED_PROMPT_COUPLE = \"A couple of students smiling to the camera, there are green cats in the background.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils.hooks\n",
    "reload(utils.hooks)\n",
    "import flux.utils\n",
    "reload(flux.utils)\n",
    "set_flux_context(pipe, dtype)\n",
    "from flux.utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import visualization\n",
    "reload(visualization)\n",
    "\n",
    "cache_pirate = single_layer_ablation_with_cache(Ablation.get_ablation(\"nothing\"), prompt=PROMPT_PIRATE, layer=2, vanilla_prompt=PROMPT_PIRATE)[1]\n",
    "display(interactive_image_activation(cache_pirate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import LogNorm\n",
    "\n",
    "# Config\n",
    "num_layers = 57\n",
    "layers = np.arange(num_layers)\n",
    "\n",
    "# Your actual norm values per layer\n",
    "norms_image_tokens = [norm_tokens(image_stream + image_act) for image_stream, image_act in zip(cache_pirate[\"image_residual\"],  cache_pirate[\"image_activation\"])]\n",
    "norms_image_tokens += [norm_tokens((text_image_stream + text_image_act)[:, 512:, :]) for text_image_stream, text_image_act in zip(cache_pirate[\"text_image_residual\"], cache_pirate[\"text_image_activation\"])]\n",
    "\n",
    "assert len(norms_image_tokens) == num_layers\n",
    "\n",
    "# Binning\n",
    "norm_min = 1      # avoid 0 for log scale y-axis\n",
    "norm_max = 40000  # based on your max\n",
    "num_bins = 300\n",
    "norm_bins = np.linspace(norm_min, norm_max, num_bins + 1)  # linear bins for now\n",
    "# Optionally switch to log bins for better alignment:\n",
    "# norm_bins = np.logspace(np.log10(norm_min), np.log10(norm_max), num_bins + 1)\n",
    "\n",
    "# Histogram matrix\n",
    "density = np.zeros((num_bins, num_layers))\n",
    "for i, values in enumerate(norms_image_tokens):\n",
    "    hist, _ = np.histogram(values, bins=norm_bins)\n",
    "    density[:, i] = hist\n",
    "\n",
    "# Plot\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "im = ax.imshow(\n",
    "    density,\n",
    "    origin='lower',\n",
    "    aspect='auto',\n",
    "    extent=[layers[0], layers[-1], norm_bins[0], norm_bins[-1]],\n",
    "    cmap='magma',\n",
    "    norm=LogNorm(vmin=1, vmax=np.max(density)),\n",
    "    interpolation='nearest'\n",
    ")\n",
    "\n",
    "# ax.set_yscale('log')  # back to log y-axis\n",
    "ax.set_xlabel('Layer')\n",
    "ax.set_ylabel('Norm')\n",
    "ax.set_title('Histogram of Norms per Layer')\n",
    "fig.colorbar(im, ax=ax, label='Log Count')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import LogNorm\n",
    "\n",
    "# Config\n",
    "num_layers = 57\n",
    "layers = np.arange(num_layers)\n",
    "\n",
    "# Your actual norm values per layer\n",
    "norms_text_tokens = [norm_tokens(image_stream + image_act) for image_stream, image_act in zip(cache_pirate[\"text_residual\"],  cache_pirate[\"text_activation\"])]\n",
    "norms_text_tokens += [norm_tokens((text_image_stream + text_image_act)[:, :512, :]) for text_image_stream, text_image_act in zip(cache_pirate[\"text_image_residual\"], cache_pirate[\"text_image_activation\"])]\n",
    "\n",
    "assert len(norms_image_tokens) == num_layers\n",
    "\n",
    "# Binning\n",
    "norm_min = 1      # avoid 0 for log scale y-axis\n",
    "norm_max = 70000  # based on your max\n",
    "num_bins = 300\n",
    "norm_bins = np.linspace(norm_min, norm_max, num_bins + 1)  # linear bins for now\n",
    "# Optionally switch to log bins for better alignment:\n",
    "# norm_bins = np.logspace(np.log10(norm_min), np.log10(norm_max), num_bins + 1)\n",
    "\n",
    "# Histogram matrix\n",
    "density = np.zeros((num_bins, num_layers))\n",
    "for i, values in enumerate(norms_text_tokens):\n",
    "    hist, _ = np.histogram(values, bins=norm_bins)\n",
    "    density[:, i] = hist\n",
    "\n",
    "# Plot\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "im = ax.imshow(\n",
    "    density,\n",
    "    origin='lower',\n",
    "    aspect='auto',\n",
    "    extent=[layers[0], layers[-1], norm_bins[0], norm_bins[-1]],\n",
    "    cmap='magma',\n",
    "    norm=LogNorm(vmin=1, vmax=np.max(density)),\n",
    "    interpolation='nearest'\n",
    ")\n",
    "\n",
    "# ax.set_yscale('log')  # back to log y-axis\n",
    "ax.set_xlabel('Layer')\n",
    "ax.set_ylabel('Norm')\n",
    "ax.set_title('Histogram of text-tokens Norms per Layer')\n",
    "fig.colorbar(im, ax=ax, label='Log Count')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's have a look at Queries and keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from attention_cache import set_cached_attention_processor\n",
    "\n",
    "attn_cache = set_cached_attention_processor(pipe)\n",
    "output_cache = single_layer_ablation_with_cache(Ablation.get_ablation(\"nothing\"), prompt=PROMPT_PIRATE, layer=2, vanilla_prompt=PROMPT_PIRATE)[1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Rectangle\n",
    "import torch.nn.functional as F\n",
    "from visualization import plot_activation_by_layer, plot_activation_by_layer_og_ablated, interactive_image_activation, norm_tokens\n",
    "\n",
    "%matplotlib widget\n",
    "\n",
    "\n",
    "def interactive_query_key_visualization(output_cache, attn_cache, layer: int):\n",
    "\n",
    "\n",
    "    query = attn_cache[\"query\"][layer]\n",
    "    key = attn_cache['key'][layer]\n",
    "\n",
    "    B, H, T, D = key.shape\n",
    "    # Create identity as the value\n",
    "    # shape: [1, 24, 4096, 4096]\n",
    "    I = torch.eye(T, device=key.device, dtype=key.dtype).expand(B, H, T, T)\n",
    "    attn_weights = F.scaled_dot_product_attention(query, key, I)  # shape: [1, 24, 4096, 4096]\n",
    "\n",
    "\n",
    "\n",
    "    # Dummy shapes — replace with your actual data\n",
    "    H, W = 64, 64\n",
    "    num_heads = 24\n",
    "\n",
    "    # Query image to select from\n",
    "    if layer >= 19:\n",
    "        query_image = norm_tokens(\n",
    "            output_cache[\"text_image_residual\"][layer - 19][:, 512:] + output_cache[\"text_image_activation\"][layer - 19][:, 512:]\n",
    "        ).reshape(H, W)\n",
    "        query_text = norm_tokens(\n",
    "            output_cache[\"text_image_residual\"][layer - 19][:, :512] + output_cache[\"text_image_activation\"][layer - 19][:, :512]\n",
    "        ).reshape(8, 64)\n",
    "    else:\n",
    "        # Query image to select from\n",
    "        query_image = norm_tokens(\n",
    "            output_cache[\"image_residual\"][layer]\n",
    "        ).reshape(H, W)\n",
    "        query_text = norm_tokens(\n",
    "            output_cache[\"text_residual\"][layer]\n",
    "        ).reshape(8, 64)\n",
    "\n",
    "    # Main attention weights: shape [24, 4096, 4096] (head, query_token, key_token)\n",
    "    head_maps = attn_weights[0, :, :, 512:].cpu()  # shape: [24, 4096, 4096]\n",
    "    text_token_scores = attn_weights[0, :, :, :512].cpu()\n",
    "\n",
    "    # Create the figure layout\n",
    "    fig, axes = plt.subplots(5, 5, figsize=(17, 17))  # enough for 24 heads + query\n",
    "    axes = axes.flatten()\n",
    "    query_ax = axes[0]\n",
    "    query_ax.imshow(\n",
    "        query_text,\n",
    "        cmap='Blues',\n",
    "        extent=[0, 64, 64, 72], \n",
    "    )\n",
    "    query_ax.imshow(query_image, cmap=\"Reds\", extent=[0, 64, 0, 64])\n",
    "\n",
    "\n",
    "    query_ax.set_title(\"Query\")\n",
    "    # Set axis limits to match pixel centers\n",
    "    query_ax.set_xlim(0, 64)\n",
    "    query_ax.set_ylim(0, 72)\n",
    "    query_ax.set_xticks([])\n",
    "    query_ax.set_yticks([])\n",
    "    query_ax.set_xticklabels([])\n",
    "    query_ax.set_yticklabels([])\n",
    "    query_ax.grid(True)\n",
    "\n",
    "\n",
    "\n",
    "    # Visual marker on query image\n",
    "    selector = Rectangle((0, 0), 1, 1, edgecolor='lime', facecolor='none', lw=2)\n",
    "    query_ax.add_patch(selector)\n",
    "\n",
    "    # Prepare heatmap axes and images\n",
    "    heatmap_axes = axes[1:num_heads+1]\n",
    "    heatmap_images = []\n",
    "\n",
    "    # For each head: store (main_heatmap, text_heatmap)\n",
    "    for idx, ax in enumerate(heatmap_axes):\n",
    "        # Dummy data to initialize\n",
    "        main_attn = head_maps[idx, 0].reshape(H, W)\n",
    "        im_main = ax.imshow(main_attn, cmap='Reds', vmin=0, vmax=torch.max(main_attn), extent=[0, 64, 0, 64], \n",
    "\n",
    "    )\n",
    "\n",
    "        # Bottom text block: last 512 tokens → reshape (64 x 8)\n",
    "        text_vals = text_token_scores[idx, 0].reshape(8, 64)\n",
    "        im_text = ax.imshow(\n",
    "            text_vals,\n",
    "            extent=[0, 64, 64, 72],  # right below the main map\n",
    "            cmap='Blues',\n",
    "            vmin=0,\n",
    "            vmax=torch.max(text_token_scores),\n",
    "\n",
    "        )\n",
    "\n",
    "        ax.set_xlim(0, 64)\n",
    "        ax.set_ylim(0, 72)\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "        ax.set_xticklabels([])\n",
    "        ax.set_yticklabels([])\n",
    "        ax.set_title(f\"Head {idx}\")\n",
    "\n",
    "        heatmap_images.append((im_text, im_main))\n",
    "\n",
    "    # Click handler\n",
    "    def update_attention_maps(event):\n",
    "        print(\"A\")\n",
    "        if event.inaxes != query_ax:\n",
    "            return\n",
    "\n",
    "        i = H + 8 - 1 - int(event.ydata)  # Flip vertical index (0 at top)\n",
    "        j = int(event.xdata)\n",
    "        q_idx = i * W + j\n",
    "        print(f\"Selected token at ({i}, {j}) → index {q_idx}\")\n",
    "        selector.set_xy((int(event.xdata), int(event.ydata)))  # update highlight box\n",
    "\n",
    "        for idx, (im_text, im_main) in enumerate(heatmap_images):\n",
    "            updated_attn = head_maps[idx, q_idx].reshape(H, W)\n",
    "            print()\n",
    "            im_main.set_data(updated_attn)\n",
    "            im_main.set_clim(vmin=0, vmax=torch.max(updated_attn))\n",
    "\n",
    "            updated_text = text_token_scores[idx, q_idx].reshape(8, 64)\n",
    "            im_text.set_data(updated_text)\n",
    "            im_text.set_clim(vmin=0, vmax=torch.max(updated_text))\n",
    "\n",
    "            heatmap_axes[idx].set_title(f\"Head {idx} (Q={q_idx})\")\n",
    "\n",
    "        fig.canvas.draw_idle()\n",
    "\n",
    "    # Connect event\n",
    "    fig.canvas.mpl_connect(\"button_press_event\", update_attention_maps)\n",
    "\n",
    "    plt.subplots_adjust(wspace=0.1, hspace=0.5)\n",
    "    plt.tight_layout()\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interactive_query_key_visualization(output_cache, attn_cache, layer=5)\n",
    "# layer 0: completely random and dense, few zigzag ()\n",
    "# layer 1: less random, mostly zigzag (with emphasis on close positions, one straight vertical line and some red circle with small area)\n",
    "# zigzag of varying intensity and thickness, dots, vertical and horizontal lines, some cloudy/bubbles stuff \n",
    "# layer 3: noisy things, larger balls, noisy stripes, some \"single\" activations, apart from some locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interactive_query_key_visualization(output_cache, attn_cache, layer=19)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interactive_query_key_visualization(output_cache, attn_cache, layer=23)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add Extra registers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import flux_hooks\n",
    "reload(flux_hooks)\n",
    "import flux.utils\n",
    "reload(flux.utils)\n",
    "from flux.utils import *\n",
    "import flux.attention_cache\n",
    "reload(flux.attention_cache)\n",
    "from flux.attention_cache import *\n",
    "set_flux_context(pipe, dtype)\n",
    "torch.cuda.empty_cache()\n",
    "attn_cache = set_cached_attention_processor(pipe)\n",
    "\n",
    "def extract_register_cache(cache):\n",
    "\n",
    "    for key in \"residual\", \"activation\":\n",
    "        cache[f\"registers_{key}\"] = [tensor[:, 4096:, :] for tensor in cache[f\"image_{key}\"]]\n",
    "        cache[f\"image_{key}\"] = [tensor[:, :4096, :] for tensor in cache[f\"image_{key}\"]]\n",
    "\n",
    "        cache[f\"registers_{key}\"].extend([tensor[:, 4608:, :] for tensor in cache[f\"text_image_{key}\"]])\n",
    "        cache[f\"text_image_{key}\"] = [tensor[:, :4608, :] for tensor in cache[f\"text_image_{key}\"]]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output, cache_pirate = single_layer_ablation_with_cache(Ablation.get_ablation(\"add_registers\", num_registers=16), prompt=PROMPT_PIRATE, layer=2, vanilla_prompt=PROMPT_PIRATE)\n",
    "extract_register_cache(cache_pirate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output, cache_pirate = single_layer_ablation_with_cache(Ablation.get_ablation(\"nothing\"), prompt=PROMPT_PIRATE, layer=2, vanilla_prompt=PROMPT_PIRATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output.images[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attn_cache_clean = {}\n",
    "attn_cache_clean[\"query\"] = [q[:, :, :4608, :4608] for q in attn_cache[\"query\"]]\n",
    "attn_cache_clean[\"key\"] = [q[:, :, :4608, :4608] for q in attn_cache[\"key\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interactive_query_key_visualization(cache_pirate, attn_cache_clean, layer=19)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "anmari_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
