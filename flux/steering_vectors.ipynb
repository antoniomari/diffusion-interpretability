{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Add parent directory to sys.path\n",
    "parent_dir = Path.cwd().parent.resolve()\n",
    "if str(parent_dir) not in sys.path:\n",
    "    sys.path.insert(0, str(parent_dir))\n",
    "\n",
    "# Verify that the path has been added correctly\n",
    "print(sys.path[0])\n",
    "\n",
    "from diffusers import FluxPipeline\n",
    "from diffusers.models import AutoencoderTiny\n",
    "from SDLens import HookedFluxPipeline, HookedPixArtPipeline\n",
    "from SAE import SparseAutoencoder\n",
    "import torch\n",
    "import os\n",
    "\n",
    "os.environ['HF_HOME'] = '/dlabscratch1/anmari'\n",
    "os.environ['TRANSFORMERS_CACHE'] = '/dlabscratch1/anmari'\n",
    "os.environ['HF_DATASETS_CACHE'] = '/dlabscratch1/anmari'\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from importlib import reload\n",
    "from visualization import plot_activation_by_layer, plot_activation_by_layer_og_ablated, interactive_image_activation, norm_tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = HookedPixArtPipeline.from_pretrained(\n",
    "    \"PixArt-alpha/PixArt-Sigma-XL-2-1024-MS\", torch_dtype=torch.float16\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Pipeline\n",
    "from flux.utils import *\n",
    "\n",
    "\n",
    "dtype = torch.float16\n",
    "pipe = HookedFluxPipeline.from_pretrained(\n",
    "    \"black-forest-labs/FLUX.1-schnell\",\n",
    "    torch_dtype=dtype,\n",
    "    device_map=\"balanced\",\n",
    ")\n",
    "pipe.set_progress_bar_config(disable=True)\n",
    "set_flux_context(pipe, dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompts to experiment with\n",
    "SPACE_PROMPT = \"A sheep riding a cow in the space, there are planets and stars in the background.\"\n",
    "CACTUS_SKATEBOARD_PROMPT = \"A cinematic shot of a cactus on a skateboard in a hotel room.\"\n",
    "GIRL_CAT_PROMPT = \"A picture of a smiling girl with a red t-shirt holding a cat.\"\n",
    "PROMPT_PIRATE = \"A pirate with red hair smiling to the camera.\"\n",
    "PROMPT_SINGER = \"A lady singer with red hair smiling to the camera.\"\n",
    "PROMPT_COUPLE = \"A couple of students smiling to the camera.\"\n",
    "PROMPT_DOG = \"An happy husky looking at the camera with a bone in his mouth.\"\n",
    "PROMPT_CARTOON = \"An cartoonish picture of two smiling students, a male on the lft with a blue shirt and a black backpack, a female on the right has a yellow pullover\"\n",
    "EMPTY_PROMPT = \"\"\n",
    "\n",
    "REFINED_SPACE_PROMPT = \"A sheep riding a cow in the space, there is a school in background.\"\n",
    "REFINED_CACTUS_SKATEBOARD_PROMPT = \"A cinematic shot of a cactus on a skateboard.\"\n",
    "REFINED_GIRL_CAT_PROMPT = \"A picture of a smiling girl with a red t-shirt holding a cat in a park.\"\n",
    "REFINED_PROMPT_PIRATE = \"A pirate with red hair smiling to the camera on a pirate sheep.\"\n",
    "REFINED_PROMPT_COUPLE = \"A couple of students smiling to the camera, there are green cats in the background.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils.hooks\n",
    "reload(utils.hooks)\n",
    "import flux.utils\n",
    "reload(flux.utils)\n",
    "set_flux_context(pipe, dtype)\n",
    "from flux.utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_mean_activations(prompt: str, seeds: list):\n",
    "    caches = []\n",
    "    for i in seeds:\n",
    "    #for i in [421]:\n",
    "        #for prompt in [\"big dog\", \"small dog\", \"brown dog\", \"puppy dog\", \"white dog\", \"old dog\",  \"angry dog\", \"happy dog\"]:\n",
    "        _, cache_gen = single_layer_ablation_with_cache(Ablation.get_ablation(\"nothing\"), prompt=prompt, layer=2, vanilla_prompt=prompt, ablated_seed=i)\n",
    "        caches.append(cache_gen)\n",
    "\n",
    "    mean_cache = {}\n",
    "    for cache in caches:\n",
    "        input_image = cache[\"image_residual\"][0]\n",
    "        input_text = cache[\"text_residual\"][0]\n",
    "        cache[\"cum_image_activation\"] = [res + act - input_image for res, act in zip(cache[\"image_residual\"], cache[\"image_activation\"])]\n",
    "        cache[\"cum_text_activation\"] = [res + act - input_text for res, act in zip(cache[\"text_residual\"], cache[\"text_activation\"])]\n",
    "        cache[\"cum_image_activation\"].extend([res[:, 512:] + act[:, 512:] - input_image for res, act in zip(cache[\"text_image_residual\"], cache[\"text_image_activation\"])])\n",
    "        cache[\"cum_text_activation\"].extend([res[:, :512] + act[:, :512] - input_text for res, act in zip(cache[\"text_image_residual\"], cache[\"text_image_activation\"])])\n",
    "\n",
    "    # mean_cache[\"cum_image_activation\"] = {layer: torch.stack([cache[\"cum_image_activation\"][layer] for cache in caches]).mean((0, 1, 2)) for layer in range(19 + 38)}\n",
    "    mean_cache[\"cum_image_activation\"] = {layer: torch.stack([cache[\"cum_image_activation\"][layer] for cache in caches]).mean((0)) for layer in range(19 + 38)}\n",
    "    mean_cache[\"cum_text_activation\"] = {layer: torch.stack([cache[\"cum_text_activation\"][layer] for cache in caches]).mean((0)) for layer in range(19 + 38)}\n",
    "    return mean_cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cache_dog = compute_mean_activations(\"A dog\", seeds=[0])\n",
    "mean_cache_dog = compute_mean_activations(\"A pirate and a sheep\", seeds=list(range(1, 51)))\n",
    "# mean_cache_random = compute_mean_activations(\"A cat\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "steering_vectors = {i: cache_dog[\"cum_image_activation\"][i] - mean_cache_dog[\"cum_image_activation\"][i] for i in range(len(mean_cache_dog[\"cum_image_activation\"]))}\n",
    "len(steering_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "steering_vectors = {i: mean_cache_dog[\"cum_image_activation\"][i] for i in range(len(mean_cache_dog[\"cum_image_activation\"]))}\n",
    "len(steering_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "steering_vectors = {i: mean_cache_dog[\"cum_text_activation\"][i] for i in range(len(mean_cache_dog[\"cum_text_activation\"]))}\n",
    "len(steering_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "steering_vectors = {i: cache_dog[\"cum_image_activation\"][i] for i in range(len(mean_cache_dog[\"cum_image_activation\"]))}\n",
    "len(steering_vectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Steering vector recap\n",
    "1. Average over seeds -> cumulative activations of shape [W, H, Hidden]\n",
    "2. Average over seeds and patches -> [Hidden, ]\n",
    "\n",
    "\n",
    "As for SDXL -> option 2 worked \n",
    "As for FLUX -> option 2 doesn't work, option 1 kinda worked\n",
    "\n",
    "Question: why doesn't work with averaging location but works without?\n",
    "\n",
    "As for SDXL, hypothesis is that the \"dog\" vector component is present in many locations, so that averaging keeps it.\n",
    "As for FLUX, if the \"dog\" vector component is present in few location (e.g. registers), this will fade out in the average, so it doesn't work anymore.\n",
    "\n",
    "\n",
    "# Now thing to do\n",
    "For this example (one seed patching, steering vector on one seed as well, steering vector on mutliple seeds)\n",
    "- Aim: find subset of locations for which we can inject the dog -> show that dog steering vector is centralized somewhere.\n",
    "- Then: if first works, we can design the experiments for SAEs according to this theory/hypothesis.\n",
    "- Read paper about Slot-attention\n",
    "\n",
    "For remote future: Pixart, SD3, Nvidia Sana "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try with multiple seeds\n",
    "# Destroy registers on multiple prompts and seeds\n",
    "\n",
    "images = []\n",
    "labels = []\n",
    "prompt = GIRL_CAT_PROMPT\n",
    "\n",
    "out_og, _ = single_layer_ablation_with_cache(Ablation.get_ablation(\"nothing\"), \n",
    "                                                                prompt=prompt,\n",
    "                                                                layer=0, \n",
    "                                                                vanilla_prompt=\"\",\n",
    "                                                                vanilla_seed=421, \n",
    "                                                                ablated_seed=421)\n",
    "images.append(out_og.images[0])\n",
    "labels.append(\"Original\")\n",
    "\n",
    "def add_steering_vector(input: torch.Tensor, output: torch.Tensor, layer: int, weight=1):\n",
    "    weighted_steering_vector = (steering_vectors[layer].to(output.device)) # - steering_vectors[min(layer, 0)].to(output.device))\n",
    "\n",
    "    # Compute L2 norm across last dim\n",
    "    norms = torch.norm(weighted_steering_vector, dim=2)  # shape: [1, 4096]\n",
    "    # Get top-N indices\n",
    "    N = 1  # change as needed\n",
    "    _, topk_indices = torch.topk(norms, 1, dim=1)\n",
    "    _, lowk_indices = torch.topk(-norms, N, dim=1)\n",
    "    # Create a mask of the same shape as norms, filled with False\n",
    "    top_mask = torch.zeros_like(norms, dtype=torch.bool)\n",
    "    top_mask = top_mask.scatter_(1, topk_indices, True).unsqueeze(-1).expand_as(weighted_steering_vector)  # shape: [1, 4096, 3072]\n",
    "    bottom_mask = torch.zeros_like(norms, dtype=torch.bool)\n",
    "    bottom_mask = bottom_mask.scatter_(1, lowk_indices, True).unsqueeze(-1).expand_as(weighted_steering_vector)  # shape: [1, 4096, 3072]\n",
    "\n",
    "    # Expand mask to match original tensor shape\n",
    "    # Zero out everything except the top-N vectors\n",
    "    weighted_steering_vector = weight * (weighted_steering_vector) # weighted_steering_vector * top_mask + weighted_steering_vector * bottom_mask  # shape: [1, 4096, 3072]\n",
    "\n",
    "    # decide how to select the components of the steering vecto\n",
    "    return input + (1 * (output - input) + 3 * weighted_steering_vector)\n",
    "\n",
    "def add_activation(input: torch.Tensor, output: torch.Tensor, layer: int):\n",
    "    \n",
    "    if layer > 0:\n",
    "        steering_vector = (steering_vectors[layer].to(output.device) - steering_vectors[layer - 1].to(output.device))\n",
    "    else:\n",
    "        steering_vector = steering_vectors[layer].to(output.device)\n",
    "\n",
    "    # decide how to select the components of the steering vector\n",
    "    return output + 0.6 * steering_vector.mean(dim=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "for layer in range(4, 57):\n",
    "\n",
    "    # generate dog -> get activation = steering_vector? \n",
    "\n",
    "\n",
    "    \n",
    "    # same \n",
    "    out_ablated, _ = single_layer_ablation_with_cache(Ablation.get_ablation(\"edit_streams\", \n",
    "                                                                            edit_fn=add_activation,\n",
    "                                                                            stream=\"image\",\n",
    "                                                                            layers = list(range(3, layer))),\n",
    "                                                        prompt=prompt,\n",
    "                                                        vanilla_prompt=\"\",\n",
    "                                                        vanilla_seed=421, \n",
    "                                                        ablated_seed=421)\n",
    "    images.append(out_ablated.images[0])\n",
    "    labels.append(f\"layer {layer}\")\n",
    "\n",
    "plot_images_grid(images, labels, nrows=6, ncols=10, figsize=(30, 18))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_og, _ = single_layer_ablation_with_cache(Ablation.get_ablation(\"nothing\"), \n",
    "                                                                prompt=\"A dog\",\n",
    "                                                                layer=0, \n",
    "                                                                vanilla_prompt=\"\",\n",
    "                                                                vanilla_seed=0, \n",
    "                                                                ablated_seed=0)\n",
    "\n",
    "out_og.images[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try with multiple seeds\n",
    "# Destroy registers on multiple prompts and seeds\n",
    "\n",
    "images = []\n",
    "labels = []\n",
    "prompt = GIRL_CAT_PROMPT\n",
    "\n",
    "out_og, _ = single_layer_ablation_with_cache(Ablation.get_ablation(\"nothing\"), \n",
    "                                                                prompt=prompt,\n",
    "                                                                layer=0, \n",
    "                                                                vanilla_prompt=\"\",\n",
    "                                                                vanilla_seed=421, \n",
    "                                                                ablated_seed=421)\n",
    "images.append(out_og.images[0])\n",
    "labels.append(\"Original\")\n",
    "\n",
    "\n",
    "for layer in range(0, 57):\n",
    "\n",
    "    # generate dog -> get activation = steering_vector? \n",
    "\n",
    "\n",
    "    \n",
    "    # same \n",
    "    out_ablated, _ = single_layer_ablation_with_cache(Ablation.get_ablation(\"edit_streams\", \n",
    "                                                                            edit_fn=partial(add_steering_vector, weight=1),\n",
    "                                                                            stream=\"image\",\n",
    "                                                                            layers=[layer]), \n",
    "                                                        prompt=prompt,\n",
    "                                                        vanilla_prompt=\"\",\n",
    "                                                        vanilla_seed=421, \n",
    "                                                        ablated_seed=421)\n",
    "    images.append(out_ablated.images[0])\n",
    "    labels.append(f\"layer {layer}\")\n",
    "\n",
    "plot_images_grid(images, labels, nrows=6, ncols=10, figsize=(30, 18))\n",
    "\n",
    "# gradient \n",
    "# shap lime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try with multiple seeds\n",
    "# Destroy registers on multiple prompts and seeds\n",
    "\n",
    "images = []\n",
    "labels = []\n",
    "prompt = GIRL_CAT_PROMPT\n",
    "\n",
    "out_og, _ = single_layer_ablation_with_cache(Ablation.get_ablation(\"nothing\"), \n",
    "                                                                prompt=prompt,\n",
    "                                                                layer=0, \n",
    "                                                                vanilla_prompt=\"\",\n",
    "                                                                vanilla_seed=421, \n",
    "                                                                ablated_seed=421)\n",
    "images.append(out_og.images[0])\n",
    "labels.append(\"Original\")\n",
    "\n",
    "\n",
    "for layer in range(0, 57):\n",
    "\n",
    "    # generate dog -> get activation = steering_vector? \n",
    "\n",
    "\n",
    "    \n",
    "    # same \n",
    "    out_ablated, _ = single_layer_ablation_with_cache(Ablation.get_ablation(\"edit_streams\", \n",
    "                                                                            edit_fn=partial(add_steering_vector, weight=1),\n",
    "                                                                            stream=\"image\",\n",
    "                                                                            layers=[layer]), \n",
    "                                                        prompt=prompt,\n",
    "                                                        vanilla_prompt=\"\",\n",
    "                                                        vanilla_seed=421, \n",
    "                                                        ablated_seed=421)\n",
    "    images.append(out_ablated.images[0])\n",
    "    labels.append(f\"layer {layer}\")\n",
    "\n",
    "plot_images_grid(images, labels, nrows=6, ncols=10, figsize=(30, 18))\n",
    "\n",
    "# gradient \n",
    "# shap lime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try with multiple seeds\n",
    "# Destroy registers on multiple prompts and seeds\n",
    "\n",
    "images = []\n",
    "labels = []\n",
    "prompt = GIRL_CAT_PROMPT\n",
    "\n",
    "out_og, _ = single_layer_ablation_with_cache(Ablation.get_ablation(\"nothing\"), \n",
    "                                                                prompt=prompt,\n",
    "                                                                layer=0, \n",
    "                                                                vanilla_prompt=\"\",\n",
    "                                                                vanilla_seed=421, \n",
    "                                                                ablated_seed=421)\n",
    "images.append(out_og.images[0])\n",
    "labels.append(\"Original\")\n",
    "\n",
    "\n",
    "for layer in range(0, 57):\n",
    "\n",
    "    # generate dog -> get activation = steering_vector? \n",
    "\n",
    "\n",
    "    \n",
    "    # same \n",
    "    out_ablated, _ = single_layer_ablation_with_cache(Ablation.get_ablation(\"edit_streams\", \n",
    "                                                                            edit_fn=partial(add_steering_vector, weight=3),\n",
    "                                                                            stream=\"image\",\n",
    "                                                                            layers=[layer]), \n",
    "                                                        prompt=prompt,\n",
    "                                                        vanilla_prompt=\"\",\n",
    "                                                        vanilla_seed=421, \n",
    "                                                        ablated_seed=421)\n",
    "    images.append(out_ablated.images[0])\n",
    "    labels.append(f\"layer {layer}\")\n",
    "\n",
    "plot_images_grid(images, labels, nrows=6, ncols=10, figsize=(30, 18))\n",
    "\n",
    "# gradient \n",
    "# shap lime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_og, _ = single_layer_ablation_with_cache(Ablation.get_ablation(\"nothing\"), \n",
    "                                                                prompt=\"A photo of a girl holding a dog\",\n",
    "                                                                layer=0, \n",
    "                                                                vanilla_prompt=\"\",\n",
    "                                                                vanilla_seed=421, \n",
    "                                                                ablated_seed=421)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_og.images[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try with multiple seeds\n",
    "# Destroy registers on multiple prompts and seeds\n",
    "\n",
    "images = []\n",
    "labels = []\n",
    "prompt = GIRL_CAT_PROMPT\n",
    "\n",
    "out_og, _ = single_layer_ablation_with_cache(Ablation.get_ablation(\"nothing\"), \n",
    "                                                                prompt=prompt,\n",
    "                                                                layer=0, \n",
    "                                                                vanilla_prompt=\"\",\n",
    "                                                                vanilla_seed=421, \n",
    "                                                                ablated_seed=421)\n",
    "images.append(out_og.images[0])\n",
    "labels.append(\"Original\")\n",
    "\n",
    "def add_steering_vector(input: torch.Tensor, output: torch.Tensor):\n",
    "    weighted_steering_vector = (steering_vectors[layer].to(output.device) - steering_vectors[min(layer, 3)].to(output.device))\n",
    "\n",
    "    # Compute L2 norm across last dim\n",
    "    norms = torch.norm(weighted_steering_vector, dim=2)  # shape: [1, 4096]\n",
    "    # Get top-N indices\n",
    "    N = 4095  # change as needed\n",
    "    _, topk_indices = torch.topk(norms, 1, dim=1)\n",
    "    _, lowk_indices = torch.topk(-norms, N, dim=1)\n",
    "    # Create a mask of the same shape as norms, filled with False\n",
    "    top_mask = torch.zeros_like(norms, dtype=torch.bool)\n",
    "    top_mask = top_mask.scatter_(1, topk_indices, True).unsqueeze(-1).expand_as(weighted_steering_vector)  # shape: [1, 4096, 3072]\n",
    "    bottom_mask = torch.zeros_like(norms, dtype=torch.bool)\n",
    "    bottom_mask = bottom_mask.scatter_(1, lowk_indices, True).unsqueeze(-1).expand_as(weighted_steering_vector)  # shape: [1, 4096, 3072]\n",
    "\n",
    "    # Expand mask to match original tensor shape\n",
    "    # Zero out everything except the top-N vectors\n",
    "    weighted_steering_vector = 2 * (weighted_steering_vector * top_mask + weighted_steering_vector * bottom_mask)  # shape: [1, 4096, 3072]\n",
    "\n",
    "    # decide how to select the components of the steering vector\n",
    "    return output + weighted_steering_vector\n",
    "\n",
    "for layer in range(0, 57):\n",
    "\n",
    "    # generate dog -> get activation = steering_vector? \n",
    "\n",
    "\n",
    "    \n",
    "    # same \n",
    "    out_ablated, _ = single_layer_ablation_with_cache(Ablation.get_ablation(\"edit_streams\", \n",
    "                                                                            edit_fn=add_steering_vector,\n",
    "                                                                            stream=\"image\"), \n",
    "                                                        prompt=prompt,\n",
    "                                                        layer=layer if layer <= 18 else layer-19, \n",
    "                                                        block_type=\"transformer_blocks\" if layer <= 18 else \"single_transformer_blocks\",\n",
    "                                                        vanilla_prompt=\"\",\n",
    "                                                        vanilla_seed=421, \n",
    "                                                        ablated_seed=421)\n",
    "    images.append(out_ablated.images[0])\n",
    "    labels.append(f\"layer {layer}\")\n",
    "\n",
    "plot_images_grid(images, labels, nrows=6, ncols=10, figsize=(30, 18))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec = (steering_vectors[18] - steering_vectors[3])\n",
    "\n",
    "# Step 1: reshape to [w, d, h]\n",
    "x_reshaped = vec.view(64, 64, 3072)\n",
    "\n",
    "# Step 2: compute norm along dim=2 (L2 norm over h)\n",
    "norms = torch.norm(x_reshaped, dim=2)  # shape: [w, d]\n",
    "\n",
    "# Step 3: plot as an image\n",
    "plt.imshow(norms.numpy(), cmap='viridis')\n",
    "plt.colorbar(label='L2 Norm')\n",
    "plt.title(\"Norm of [d]-vectors in [w x d] grid\")\n",
    "plt.axis('off')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try with multiple seeds\n",
    "# Destroy registers on multiple prompts and seeds\n",
    "\n",
    "images = []\n",
    "labels = []\n",
    "prompt = GIRL_CAT_PROMPT\n",
    "\n",
    "out_og, cache_og = single_layer_ablation_with_cache(Ablation.get_ablation(\"nothing\"), \n",
    "                                                                prompt=prompt,\n",
    "                                                                vanilla_prompt=\"\",\n",
    "                                                                vanilla_seed=1, \n",
    "                                                                ablated_seed=1)\n",
    "images.append(out_og.images[0])\n",
    "labels.append(\"Original\")\n",
    "\n",
    "for layer in range(0, 57):\n",
    "\n",
    "    # generate dog -> get activation = steering_vector? \n",
    "    \n",
    "    # same \n",
    "    out_ablated, _ = single_layer_ablation_with_cache(Ablation.get_ablation(\"edit_streams\", \n",
    "                                                                            edit_fn=lambda input, output, layer: cache_og[\"image_residual\"][0].to(output.device) + steering_vectors[layer].to(output.device),\n",
    "                                                                            stream=\"image\",\n",
    "                                                                            layers=[layer]), \n",
    "                                                                prompt=prompt,\n",
    "                                                                vanilla_prompt=\"\",\n",
    "                                                                vanilla_seed=1, \n",
    "                                                                ablated_seed=1)\n",
    "    images.append(out_ablated.images[0])\n",
    "    labels.append(f\"layer {layer}\")\n",
    "\n",
    "plot_images_grid(images, labels, nrows=6, ncols=10, figsize=(30, 18))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "anmari_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
