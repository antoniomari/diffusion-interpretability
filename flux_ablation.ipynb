{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers import FluxPipeline\n",
    "from diffusers.models import AutoencoderTiny\n",
    "from SDLens import HookedFluxPipeline\n",
    "from SAE import SparseAutoencoder\n",
    "from utils import add_feature_on_area, replace_with_feature\n",
    "import torch\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from importlib import reload"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Understand how FLUX works\n",
    "1. Ablations\n",
    "2. Activation patching: forward pass (store activation), forward pass 2 -> replace activation\n",
    "3. Ablating different timesteps: \n",
    "\n",
    "# Papers\n",
    "1. [Done] Transformer diffusion\n",
    "2. [No paper] FLUX\n",
    "3. See the post explaining SDXL latent-space\n",
    "3. KV-edit\n",
    "4. ConceptAttention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Pipeline\n",
    "from flux.utils import *\n",
    "\n",
    "dtype = torch.float16 # torch.float32\n",
    "pipe = HookedFluxPipeline.from_pretrained(\n",
    "    \"black-forest-labs/FLUX.1-schnell\",\n",
    "    torch_dtype=dtype,\n",
    "#    device_map=\"balanced\",\n",
    "    # variant=(\"fp16\" if dtype==torch.float16 else None)\n",
    ")\n",
    "# pipe.pipe.vae = AutoencoderTiny.from_pretrained(\"madebyollin/taef1\", torch_dtype=dtype)\n",
    "pipe.pipe.enable_sequential_cpu_offload()\n",
    "pipe.set_progress_bar_config(disable=True)\n",
    "\n",
    "set_flux_context(pipe, dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "('base_image_seq_len', 256),\n",
    "('max_image_seq_len', 4096),\n",
    "('base_shift', 0.5),\n",
    "('max_shift', 1.15),\n",
    "\n",
    "pipe.pipe.transformer.config\\\n",
    "In channels: 64\n",
    "Inner dim: 3072\n",
    "\n",
    "x (64 channel) -> Linear to 3072 channels\n",
    "\n",
    "\n",
    "query_dim=3072,\n",
    "cross_attention_dim=None,\n",
    "added_kv_proj_dim=3072,\n",
    "dim_head=128,\n",
    "heads=24,\n",
    "out_dim=3072,\n",
    "context_pre_only=False\n",
    "bias=True\n",
    "processor=FluxAttnProcessor2_0()\n",
    "qk_norm=\"rms_norm\"\n",
    "eps=1e-6\n",
    "\n",
    "\n",
    "### Then\n",
    "norm_q = RMSNorm(dim_head, eps=eps)\n",
    "norm_k = RMSNorm(dim_head, eps=eps)\n",
    "norm_cross = None\n",
    "only_cross_attention = False\n",
    "\n",
    "to_k = Linear(3072, 3072)\n",
    "to_v = Linear(3072, 3072)\n",
    "add_k_proj = Linear(3072, 3072)\n",
    "add_v_proj = Linear(3072, 3072)\n",
    "add_q_proj = Linear(3072, 3072)\n",
    "\n",
    "to_out = Linear(3072, 3072)\n",
    "to_add_out = Lonear(3072, 3072)\n",
    "\n",
    "norm_added_q = RMSNorm(dim_head, eps=eps)\n",
    "norm_added_k = RMSNorm(dim_head, eps=eps)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start srsly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_params = sum(p.numel() for p in pipe.pipe.transformer.parameters())\n",
    "trainable_params = sum(p.numel() for p in pipe.pipe.transformer.parameters() if p.requires_grad)\n",
    "non_trainable_params = num_params - trainable_params\n",
    "\n",
    "print(f\"Trainable parameters: {trainable_params:,}\")\n",
    "print(f\"Non-trainable parameters: {non_trainable_params:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt=\"A cinematic shot of a professor sloth wearing a tuxedo at a BBQ party.\"\n",
    "prompt=\"A cinematic shot of a unicorn walking on a rainbow.\"\n",
    "\n",
    "with torch.no_grad():\n",
    "    output = pipe.run_with_hooks(\n",
    "        prompt=prompt,\n",
    "        position_hook_dict={},\n",
    "        num_inference_steps=1,\n",
    "        guidance_scale=0.0,\n",
    "        width=1024,\n",
    "        height=1024,\n",
    "        generator=torch.Generator(device=\"cpu\").manual_seed(1)\n",
    "    )\n",
    "    display(output.images[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt=\"A cinematic shot of a professor sloth wearing a tuxedo at a BBQ party.\"\n",
    "prompt=\"A cinematic shot of a unicorn walking on a rainbow.\"\n",
    "output = pipe.run_with_hooks(\n",
    "    prompt=prompt,\n",
    "    position_hook_dict={},\n",
    "    num_inference_steps=1,\n",
    "    guidance_scale=0.0,\n",
    "    width=1024,\n",
    "    height=1024,\n",
    "    generator=torch.Generator(device=\"cpu\").manual_seed(42)\n",
    ")\n",
    "output.images[0] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "def plot_images_grid(image_rows, title_rows, nrows, ncols, figsize=(10, 10)):\n",
    "    \"\"\"\n",
    "    Plots a grid of images with corresponding titles from a list of lists.\n",
    "\n",
    "    :param image_rows: List of lists containing PIL.Image.Image objects (each inner list is a row)\n",
    "    :param title_rows: List of lists containing titles corresponding to the images\n",
    "    :param figsize: Tuple specifying figure size\n",
    "    \"\"\"\n",
    "\n",
    "    image_rows = [image_rows[ncols * j : ncols*(j+1)] for j in range(nrows)]\n",
    "    title_rows = [title_rows[ncols * j : ncols*(j+1)] for j in range(nrows)]\n",
    "\n",
    "\n",
    "    rows = len(image_rows)  # Number of rows\n",
    "    cols = max(len(row) for row in image_rows)  # Maximum number of columns\n",
    "\n",
    "    fig, axes = plt.subplots(rows, cols, figsize=figsize)\n",
    "\n",
    "    # Ensure axes is always a 2D array, even if there's only one row or column\n",
    "    if rows == 1:\n",
    "        axes = [axes]  # Convert 1D array to 2D list\n",
    "    if cols == 1:\n",
    "        axes = [[ax] for ax in axes]  # Convert 1D array to 2D list\n",
    "\n",
    "    for r, (img_row, title_row) in enumerate(zip(image_rows, title_rows)):\n",
    "        for c, (img, title) in enumerate(zip(img_row, title_row)):\n",
    "            axes[r][c].imshow(img)\n",
    "            axes[r][c].set_title(title)\n",
    "            axes[r][c].axis(\"off\")\n",
    "\n",
    "    # Hide unused subplots (in case of uneven rows)\n",
    "    for r in range(rows):\n",
    "        for c in range(len(image_rows[r]), cols):\n",
    "            axes[r][c].axis(\"off\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images, titles = ablate_transformer_blocks(prompt=\"A cinematic shot of a unicorn walking on a rainbow.\",\n",
    "                                            width=1024, height=1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images_grid = [images[0][4*i:4*(i+1)] for i in range(5)]\n",
    "titles_grid = [titles[0][4*i:4*(i+1)] for i in range(5)]\n",
    "# reshapwe\n",
    "plot_images_grid(images_grid, titles_grid, figsize=(20, 20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images_single, titles_single = ablate_transformer_blocks(prompt=prompt, block_type=\"single_transformer_blocks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images_single_grid = [images_single[0][6*i:6*(i+1)] for i in range(7)]\n",
    "titles_single_grid = [titles_single[0][6*i:6*(i+1)] for i in range(7)]\n",
    "# reshapwe\n",
    "plot_images_grid(images_single_grid, titles_single_grid, figsize=(25, 25))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import random\n",
    "# selection = random.sample([i for i in range(3, 19)], 1)\n",
    "chunk_images = []\n",
    "chunk_labels = []\n",
    "\n",
    "for end in range(3, 20):\n",
    "    img, lbl = ablate_block_chunk(block_type=\"transformer_blocks\", blocks_idx=[i for i in range(3, end)])\n",
    "    chunk_images.append(img)\n",
    "    chunk_labels.append(lbl)\n",
    "\n",
    "\n",
    "# ablating 16, 18 -> grey\n",
    "# 8, 11 -> broken concepts\n",
    "# 4, 13 -> broken concepts\n",
    "# 4, 6 -> broken concepts\n",
    "# 12, 15 -> broken concepts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import random\n",
    "# selection = random.sample([i for i in range(3, 19)], 1)\n",
    "chunk_images = []\n",
    "chunk_labels = []\n",
    "\n",
    "for start in range(18, 2, -1):\n",
    "    img, lbl = ablate_block_chunk(block_type=\"transformer_blocks\", blocks_idx=[i for i in range(start, 19)])\n",
    "    chunk_images.append(img)\n",
    "    chunk_labels.append(lbl)\n",
    "\n",
    "\n",
    "# ablating 16, 18 -> grey\n",
    "# 8, 11 -> broken concepts\n",
    "# 4, 13 -> broken concepts\n",
    "# 4, 6 -> broken concepts\n",
    "# 12, 15 -> broken concepts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_images_grid([chunk_images[4*i:4*(i+1)] for i in range(5)], [chunk_labels[4*i:4*(i+1)] for i in range(5)], figsize=(30, 30))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import random\n",
    "# selection = random.sample([i for i in range(3, 19)], 1)\n",
    "chunk_images = []\n",
    "chunk_labels = []\n",
    "\n",
    "for end in range(3, 20):\n",
    "    img, lbl = ablate_block_chunk(prompt=prompt, block_type=\"transformer_blocks\", blocks_idx=[i for i in range(max(3, end - 7), end)])\n",
    "    chunk_images.append(img)\n",
    "    chunk_labels.append(lbl)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_images_grid([chunk_images[4*i:4*(i+1)] for i in range(5)], [chunk_labels[4*i:4*(i+1)] for i in range(5)], figsize=(20, 25))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_images = []\n",
    "chunk_labels = []\n",
    "\n",
    "for end in range(1, 39):\n",
    "    img, lbl = ablate_block_chunk(prompt=prompt, block_type=\"single_transformer_blocks\", blocks_idx=[i for i in range(0, end)])\n",
    "    chunk_images.append(img)\n",
    "    chunk_labels.append(lbl)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_labels = [f\"Ablating from 0 to {i}\" for i in range(38)]\n",
    "plot_images_grid([chunk_images[6*i:6*(i+1)] for i in range(7)], [chunk_labels[6*i:6*(i+1)] for i in range(7)], figsize=(25, 25))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ablate single\n",
    "images_single, titles_single = ablate_transformer_blocks(block_type=\"single_transformer_blocks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images_single_grid = [images_single[0][6*i:6*(i+1)] for i in range(7)]\n",
    "titles_single_grid = [titles_single[0][6*i:6*(i+1)] for i in range(7)]\n",
    "# reshapwe\n",
    "plot_images_grid(images_single_grid, titles_single_grid, figsize=(30, 30))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "activation_patching(prompt, 10, encoder_hidden_states=False, empty_prompt_seed=39)\n",
    "# look at seed 40, 41 -> weird noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_images = []\n",
    "chunk_labels = []\n",
    "\n",
    "for i in range(19):\n",
    "    img = activation_patching(prompt, i, encoder_hidden_states=False, empty_prompt_seed=39)\n",
    "    chunk_images.append(img)\n",
    "    chunk_labels.append(f\"patching {i}\")\n",
    "\n",
    "plot_images_grid([chunk_images[4*i:4*(i+1)] for i in range(5)], [chunk_labels[4*i:4*(i+1)] for i in range(5)], figsize=(20, 25))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_images = []\n",
    "chunk_labels = []\n",
    "\n",
    "for i in range(19):\n",
    "    img = activation_patching(prompt, i, encoder_hidden_states=True)\n",
    "    chunk_images.append(img)\n",
    "    chunk_labels.append(f\"patching {i}\")\n",
    "\n",
    "plot_images_grid([chunk_images[4*i:4*(i+1)] for i in range(5)], [chunk_labels[4*i:4*(i+1)] for i in range(5)], figsize=(20, 25))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_images = []\n",
    "chunk_labels = []\n",
    "\n",
    "for i in range(38):\n",
    "    img = activation_patching(prompt, i, block_type=\"single_transformer_blocks\")\n",
    "    chunk_images.append(img)\n",
    "    chunk_labels.append(f\"patching {i}\")\n",
    "\n",
    "plot_images_grid([chunk_images[6*i:6*(i+1)] for i in range(7)], [chunk_labels[6*i:6*(i+1)] for i in range(7)], figsize=(25, 25))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_images = []\n",
    "chunk_labels = []\n",
    "\n",
    "for i in range(19):\n",
    "    img = activation_patching(prompt=\"A smiling girl\", i=i, encoder_hidden_states=True)\n",
    "    chunk_images.append(img)\n",
    "    chunk_labels.append(f\"patching {i}\")\n",
    "\n",
    "plot_images_grid([chunk_images[4*i:4*(i+1)] for i in range(5)], [chunk_labels[4*i:4*(i+1)] for i in range(5)], figsize=(20, 25))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_images = []\n",
    "chunk_labels = []\n",
    "\n",
    "for i in range(38):\n",
    "    img = activation_patching(\"A cinematic shot of a professor sloth wearing a tuxedo at a BBQ party.\", i, \"single_transformer_blocks\")\n",
    "    chunk_images.append(img)\n",
    "    chunk_labels.append(f\"patching {i}\")\n",
    "\n",
    "plot_images_grid([chunk_images[6*i:6*(i+1)] for i in range(7)], [chunk_labels[6*i:6*(i+1)] for i in range(7)], figsize=(25, 25))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtype = torch.float16 # torch.float32\n",
    "pipe = HookedFluxPipeline.from_pretrained(\n",
    "    \"black-forest-labs/FLUX.1-schnell\",\n",
    "    torch_dtype=dtype,\n",
    "    device_map=\"balanced\",\n",
    "    # variant=(\"fp16\" if dtype==torch.float16 else None)\n",
    ")\n",
    "# pipe.pipe.vae = AutoencoderTiny.from_pretrained(\"madebyollin/taef1\", torch_dtype=dtype)\n",
    "# pipe.pipe.enable_sequential_cpu_offload()\n",
    "pipe.set_progress_bar_config(disable=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    output = pipe.run_with_hooks(\n",
    "        prompt=[\"\"] * 10,\n",
    "        position_hook_dict={},\n",
    "        num_inference_steps=1,\n",
    "        guidance_scale=0.0,\n",
    "        width=1024,\n",
    "        height=1024,\n",
    "        generator=[torch.Generator(device=\"cpu\").manual_seed(i) for i in range(10)]\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    output = pipe.run_with_hooks(\n",
    "        prompt=[\"\"],\n",
    "        position_hook_dict={},\n",
    "        num_inference_steps=1,\n",
    "        guidance_scale=0.0,\n",
    "        width=1024,\n",
    "        height=1024,\n",
    "        generator=torch.Generator(device=\"cpu\").manual_seed(0)\n",
    "    )\n",
    "output.images[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_images_grid(output.images, [f\"seed {i}\" for i in range(10)], 3, 4, (20, 20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import flux.utils\n",
    "reload(flux.utils)\n",
    "from flux.utils import *\n",
    "set_flux_context(pipe, dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"A cinematic shot of a unicorn walking on a rainbow.\"\n",
    "torch.cuda.empty_cache()\n",
    "output = activation_patching(prompt, 0, encoder_hidden_states=True, empty_prompt_seed=[i for i in range(10)])\n",
    "plot_images_grid(output.images, [f\"seed {i}\" for i in range(10)], 3, 4, (20, 20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"A cinematic shot of a unicorn walking on a rainbow.\"\n",
    "torch.cuda.empty_cache()\n",
    "output = activation_patching(prompt, 0, encoder_hidden_states=True, \n",
    "                             empty_prompt_seed=[i for i in range(10, 30)],\n",
    "                             prompt_seed=[i for i in range(10, 30)])\n",
    "plot_images_grid(output.images, [f\"seed {i}\" for i in range(10, 30)], 5, 4, (20, 20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt=\"A cinematic shot of a unicorn walking on a rainbow.\"\n",
    "output = activation_patching(prompt, 0, encoder_hidden_states=False, empty_prompt_seed=[i for i in range(10)])\n",
    "plot_images_grid(output.images, [f\"seed {i}\" for i in range(10)], 3, 4, (20, 20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils.hooks\n",
    "reload(utils.hooks)\n",
    "from utils.hooks import PromptCachePreForwardHook\n",
    "\n",
    "\n",
    "def prompt_patching(prompt: str, i: int, block_type: Literal[\"transformer_blocks\", \"single_transformer_blocks\"] = \"transformer_blocks\",\n",
    "                    empty_prompt_seed=42, prompt_seed=42, second_prompt: str = None):\n",
    "    \n",
    "    if second_prompt is None:\n",
    "        second_prompt = \"\"\n",
    "        \n",
    "    if type(empty_prompt_seed) == list:\n",
    "        empty_generators = [torch.Generator(device=\"cpu\").manual_seed(j) for j in empty_prompt_seed]\n",
    "        empty_prompt = [second_prompt] * len(empty_prompt_seed)\n",
    "        prompt = [prompt] * len(empty_prompt_seed)\n",
    "    else:\n",
    "        empty_generators = torch.Generator(device=\"cpu\").manual_seed(empty_prompt_seed)\n",
    "        empty_prompt = second_prompt\n",
    "\n",
    "    if type(prompt_seed) == list:\n",
    "        generators = [torch.Generator(device=\"cpu\").manual_seed(j) for j in prompt_seed]\n",
    "    else:\n",
    "        generators = torch.Generator(device=\"cpu\").manual_seed(prompt_seed)\n",
    "\n",
    "    with torch.autocast(device_type=\"cuda\", dtype=dtype):\n",
    "        with torch.no_grad():\n",
    "            attn_cache = PromptCachePreForwardHook()\n",
    "\n",
    "            output_empty_prompt = pipe.run_with_hooks(\n",
    "                empty_prompt,\n",
    "                position_hook_dict={},\n",
    "                position_pre_hook_dict={f\"transformer.{block_type}.{i}\": attn_cache.get_hidden_states},\n",
    "                with_kwargs=True,\n",
    "                num_inference_steps=1,\n",
    "                guidance_scale=0.0,\n",
    "                generator=empty_generators,\n",
    "                width=1024,\n",
    "                height=1024,\n",
    "            )\n",
    "\n",
    "\n",
    "            output_ablated = pipe.run_with_hooks(\n",
    "                prompt,\n",
    "                position_hook_dict={},\n",
    "                position_pre_hook_dict={f\"transformer.{block_type}.{i}\": attn_cache.set_hidden_states},\n",
    "                with_kwargs=True,\n",
    "                num_inference_steps=1,\n",
    "                guidance_scale=0.0,\n",
    "                generator=generators,\n",
    "                width=1024,\n",
    "                height=1024,\n",
    "            )\n",
    "    \n",
    "    return output_ablated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt=\"A cinematic shot of a unicorn walking on a rainbow.\"\n",
    "output = prompt_patching(prompt, 0, empty_prompt_seed=[i for i in range(10)])\n",
    "plot_images_grid(output.images, [f\"seed {i}\" for i in range(10)], 3, 4, (20, 20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt=\"A cinematic shot of a unicorn walking on a rainbow.\"\n",
    "\n",
    "images = []\n",
    "labels = []\n",
    "for layer in range(10):\n",
    "    output = prompt_patching(prompt, layer, empty_prompt_seed=[i for i in range(10)])\n",
    "    images.extend(output.images)\n",
    "    labels.extend([f\"Layer {layer} \" + f\"seed {i}\" for i in range(10)])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images_resh = np.array(images).reshape(10, 10, 1024, 1024, 3).transpose(1, 0, 2, 3, 4).reshape(100, 1024, 1024, 3)\n",
    "labels_resh = np.array(labels).reshape(10, 10).T.flatten()\n",
    "plot_images_grid(images_resh, labels_resh, 10, 10, (30, 30))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt=\"A cinematic shot of a unicorn walking on a rainbow.\"\n",
    "\n",
    "images = []\n",
    "labels = []\n",
    "for layer in range(10, 19):\n",
    "    output = prompt_patching(prompt, layer, empty_prompt_seed=[i for i in range(10)])\n",
    "    images.extend(output.images)\n",
    "    labels.extend([f\"Layer {layer} \" + f\"seed {i}\" for i in range(10)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images_resh = np.array(images).reshape(9, 10, 1024, 1024, 3).transpose(1, 0, 2, 3, 4).reshape(90, 1024, 1024, 3)\n",
    "labels_resh = np.array(labels).reshape(9, 10).T.flatten()\n",
    "plot_images_grid(images_resh, labels_resh, 10, 9, (30, 30))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt=\"A cinematic shot of a unicorn walking on a rainbow.\"\n",
    "\n",
    "images = []\n",
    "labels = []\n",
    "for layer in range(0, 10):\n",
    "    output = prompt_patching(prompt, layer, block_type=\"single_transformer_blocks\", empty_prompt_seed=[i for i in range(10)])\n",
    "    images.extend(output.images)\n",
    "    labels.extend([f\"Layer {layer} \" + f\"seed {i}\" for i in range(10)])\n",
    "\n",
    "images_resh = np.array(images).reshape(10, 10, 1024, 1024, 3).transpose(1, 0, 2, 3, 4).reshape(100, 1024, 1024, 3)\n",
    "labels_resh = np.array(labels).reshape(10, 10).T.flatten()\n",
    "plot_images_grid(images_resh, labels_resh, 10, 10, (30, 30))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "second_prompt = \"A sheep riding a cow in the space, there are planets and stars in the background.\"\n",
    "\n",
    "images = []\n",
    "labels = []\n",
    "for layer in range(0, 10):\n",
    "    output = prompt_patching(prompt, layer, block_type=\"transformer_blocks\", empty_prompt_seed=[i for i in range(2)], second_prompt=second_prompt)\n",
    "    images.extend(output.images)\n",
    "    labels.extend([f\"Layer {layer} \" + f\"seed {i}\" for i in range(2)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images_resh = np.array(images).reshape(10, 2, 1024, 1024, 3).transpose(1, 0, 2, 3, 4).reshape(20, 1024, 1024, 3)\n",
    "labels_resh = np.array(labels).reshape(10, 2).T.flatten()\n",
    "plot_images_grid(images_resh, labels_resh, 2, 10, (30, 8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "second_prompt = \"A sheep riding a cow in the space, there are planets and stars in the background.\"\n",
    "\n",
    "images = []\n",
    "labels = []\n",
    "for layer in range(10, 19):\n",
    "    output = prompt_patching(prompt, layer, block_type=\"transformer_blocks\", empty_prompt_seed=[i for i in range(2)], second_prompt=second_prompt)\n",
    "    images.extend(output.images)\n",
    "    labels.extend([f\"Layer {layer} \" + f\"seed {i}\" for i in range(2)])\n",
    "\n",
    "images_resh = np.array(images).reshape(9, 2, 1024, 1024, 3).transpose(1, 0, 2, 3, 4).reshape(18, 1024, 1024, 3)\n",
    "labels_resh = np.array(labels).reshape(9, 2).T.flatten()\n",
    "plot_images_grid(images_resh, labels_resh, 2, 9, (30, 8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "second_prompt = \"A sheep riding a cow in the space, there are planets and stars in the background.\"\n",
    "\n",
    "images = []\n",
    "labels = []\n",
    "for layer in range(0, 10):\n",
    "    output = prompt_patching(prompt, layer, block_type=\"single_transformer_blocks\", empty_prompt_seed=[i for i in range(2)], second_prompt=second_prompt)\n",
    "    images.extend(output.images)\n",
    "    labels.extend([f\"Layer {layer} \" + f\"seed {i}\" for i in range(2)])\n",
    "\n",
    "images_resh = np.array(images).reshape(10, 2, 1024, 1024, 3).transpose(1, 0, 2, 3, 4).reshape(20, 1024, 1024, 3)\n",
    "labels_resh = np.array(labels).reshape(10, 2).T.flatten()\n",
    "plot_images_grid(images_resh, labels_resh, 2, 10, (30, 8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "second_prompt = \"A sheep riding a cow in the space, there are planets and stars in the background.\"\n",
    "\n",
    "images = []\n",
    "labels = []\n",
    "for layer in range(10, 20):\n",
    "    output = prompt_patching(prompt, layer, block_type=\"single_transformer_blocks\", empty_prompt_seed=[i for i in range(2)], second_prompt=second_prompt)\n",
    "    images.extend(output.images)\n",
    "    labels.extend([f\"Layer {layer} \" + f\"seed {i}\" for i in range(2)])\n",
    "\n",
    "images_resh = np.array(images).reshape(10, 2, 1024, 1024, 3).transpose(1, 0, 2, 3, 4).reshape(20, 1024, 1024, 3)\n",
    "labels_resh = np.array(labels).reshape(10, 2).T.flatten()\n",
    "plot_images_grid(images_resh, labels_resh, 2, 10, (30, 8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "second_prompt = \"A sheep riding a cow in the space, there are planets and stars in the background.\"\n",
    "\n",
    "images = []\n",
    "labels = []\n",
    "for layer in range(20, 30):\n",
    "    output = prompt_patching(prompt, layer, block_type=\"single_transformer_blocks\", empty_prompt_seed=[i for i in range(2)], second_prompt=second_prompt)\n",
    "    images.extend(output.images)\n",
    "    labels.extend([f\"Layer {layer} \" + f\"seed {i}\" for i in range(2)])\n",
    "\n",
    "images_resh = np.array(images).reshape(10, 2, 1024, 1024, 3).transpose(1, 0, 2, 3, 4).reshape(20, 1024, 1024, 3)\n",
    "labels_resh = np.array(labels).reshape(10, 2).T.flatten()\n",
    "plot_images_grid(images_resh, labels_resh, 2, 10, (30, 8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "second_prompt = \"A sheep riding a cow in the space, there are planets and stars in the background.\"\n",
    "\n",
    "images = []\n",
    "labels = []\n",
    "for layer in range(30, 38):\n",
    "    output = prompt_patching(prompt, layer, block_type=\"single_transformer_blocks\", empty_prompt_seed=[i for i in range(2)], second_prompt=second_prompt)\n",
    "    images.extend(output.images)\n",
    "    labels.extend([f\"Layer {layer} \" + f\"seed {i}\" for i in range(2)])\n",
    "\n",
    "images_resh = np.array(images).reshape(8, 2, 1024, 1024, 3).transpose(1, 0, 2, 3, 4).reshape(16, 1024, 1024, 3)\n",
    "labels_resh = np.array(labels).reshape(8, 2).T.flatten()\n",
    "plot_images_grid(images_resh, labels_resh, 2, 8, (30, 8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt=\"A cinematic shot of a unicorn walking on a rainbow.\"\n",
    "\n",
    "images = []\n",
    "labels = []\n",
    "for layer in range(10, 20):\n",
    "    output = prompt_patching(prompt, layer, block_type=\"single_transformer_blocks\", empty_prompt_seed=[i for i in range(10)])\n",
    "    images.extend(output.images)\n",
    "    labels.extend([f\"Layer {layer} \" + f\"seed {i}\" for i in range(10)])\n",
    "\n",
    "images_resh = np.array(images).reshape(10, 10, 1024, 1024, 3).transpose(1, 0, 2, 3, 4).reshape(100, 1024, 1024, 3)\n",
    "labels_resh = np.array(labels).reshape(10, 10).T.flatten()\n",
    "plot_images_grid(images_resh, labels_resh, 10, 10, (30, 30))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt=\"A cinematic shot of a unicorn walking on a rainbow.\"\n",
    "\n",
    "images = []\n",
    "labels = []\n",
    "for layer in range(20, 30):\n",
    "    output = prompt_patching(prompt, layer, block_type=\"single_transformer_blocks\", empty_prompt_seed=[i for i in range(10)])\n",
    "    images.extend(output.images)\n",
    "    labels.extend([f\"Layer {layer} \" + f\"seed {i}\" for i in range(10)])\n",
    "\n",
    "images_resh = np.array(images).reshape(10, 10, 1024, 1024, 3).transpose(1, 0, 2, 3, 4).reshape(100, 1024, 1024, 3)\n",
    "labels_resh = np.array(labels).reshape(10, 10).T.flatten()\n",
    "plot_images_grid(images_resh, labels_resh, 10, 10, (30, 30))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt=\"A cinematic shot of a unicorn walking on a rainbow.\"\n",
    "\n",
    "images = []\n",
    "labels = []\n",
    "for layer in range(30, 38):\n",
    "    output = prompt_patching(prompt, layer, block_type=\"single_transformer_blocks\", empty_prompt_seed=[i for i in range(10)])\n",
    "    images.extend(output.images)\n",
    "    labels.extend([f\"Layer {layer} \" + f\"seed {i}\" for i in range(10)])\n",
    "\n",
    "images_resh = np.array(images).reshape(8, 10, 1024, 1024, 3).transpose(1, 0, 2, 3, 4).reshape(80, 1024, 1024, 3)\n",
    "labels_resh = np.array(labels).reshape(8, 10).T.flatten()\n",
    "plot_images_grid(images_resh, labels_resh, 10, 8, (30, 30))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "anmari_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
